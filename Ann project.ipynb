{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35e4cf68-eb7f-4ff7-b97b-85bdb4f7265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb67fa4c-7697-4a1f-8c37-18e0357d21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4311\n",
      "Epoch 2, Loss: 0.1802\n",
      "Epoch 3, Loss: 0.1324\n",
      "Epoch 4, Loss: 0.1060\n",
      "Epoch 5, Loss: 0.0896\n",
      "Epoch 6, Loss: 0.0756\n",
      "Epoch 7, Loss: 0.0673\n",
      "Epoch 8, Loss: 0.0618\n",
      "Epoch 9, Loss: 0.0529\n",
      "Epoch 10, Loss: 0.0493\n",
      "Training Accuracy: 97.98 %\n",
      "Test Accuracy: 96.83 %\n",
      "Validation Accuracy: 96.83 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/salma-sulthana/ann project/train/', download=True, train=True, transform=transform)\n",
    "test_data = datasets.MNIST('/home/salma-sulthana/ann project/validation/', download=True, train=False, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/salma-sulthana/ann project/test/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / (i + 1):.4f}')\n",
    "\n",
    "# Evaluate on training set\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * train_correct / train_total:.2f} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * test_correct / test_total:.2f} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        val_total += labels.size(0)\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * val_correct / val_total:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6815cb-5825-485d-a98b-428d52cdb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efde319-5ac0-4bd6-94aa-e5758bf892ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "train_path = '/home/salma-sulthana/ann project/train/'\n",
    "validation_path = '/home/salma-sulthana/ann project/validation/'\n",
    "test_path = '/home/salma-sulthana/ann project/test/'\n",
    "\n",
    "# Load training data\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for filename in os.listdir(train_path):\n",
    "    if filename.endswith('.jpg'):  # Assuming images are in JPG format\n",
    "        img = Image.open(os.path.join(train_path, filename))\n",
    "        img = img.resize((32, 32))  # Resize image to a standard size\n",
    "        img_array = np.array(img)\n",
    "        train_images.append(img_array.flatten() / 255.0)  # Flatten and normalize image\n",
    "        train_labels.append(1 if filename.startswith('flower') else -1)  # Assuming flowers are positive class (1), others are negative class (-1)\n",
    "\n",
    "X_train = np.array(train_images)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Load validation data\n",
    "val_images = []\n",
    "val_labels = []\n",
    "for filename in os.listdir(validation_path):\n",
    "    if filename.endswith('.jpg'):  # Assuming images are in JPG format\n",
    "        img = Image.open(os.path.join(validation_path, filename))\n",
    "        img = img.resize((32, 32))  # Resize image to a standard size\n",
    "        img_array = np.array(img)\n",
    "        val_images.append(img_array.flatten() / 255.0)  # Flatten and normalize image\n",
    "        val_labels.append(1 if filename.startswith('flower') else -1)  # Assuming flowers are positive class (1), others are negative class (-1)\n",
    "\n",
    "X_val = np.array(val_images)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "# Load test data\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for filename in os.listdir(test_path):\n",
    "    if filename.endswith('.jpg'):  # Assuming images are in JPG format\n",
    "        img = Image.open(os.path.join(test_path, filename))\n",
    "        img = img.resize((32, 32))  # Resize image to a standard size\n",
    "        img_array = np.array(img)\n",
    "        test_images.append(img_array.flatten() / 255.0)  # Flatten and normalize image\n",
    "        test_labels.append(1 if filename.startswith('flower') else -1)  # Assuming flowers are positive class (1), others are negative class (-1)\n",
    "\n",
    "X_test = np.array(test_images)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Initialize perceptron weights and bias\n",
    "num_features = X_train.shape[1]\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the perceptron\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    for i in range(len(X_train)):\n",
    "        prediction = np.dot(X_train[i], weights) + bias\n",
    "        if prediction >= 0:\n",
    "            y_pred = 1\n",
    "        else:\n",
    "            y_pred = -1\n",
    "        if y_pred != y_train[i]:\n",
    "            weights += learning_rate * y_train[i] * X_train[i]\n",
    "            bias += learning_rate * y_train[i]\n",
    "\n",
    "# Evaluate on training set\n",
    "correct_train = 0\n",
    "for i in range(len(X_train)):\n",
    "    prediction = np.dot(X_train[i], weights) + bias\n",
    "    if prediction >= 0:\n",
    "        y_pred = 1\n",
    "    else:\n",
    "        y_pred = -1\n",
    "    if y_pred == y_train[i]:\n",
    "        correct_train += 1\n",
    "\n",
    "train_accuracy = correct_train / len(X_train) * 100\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct_val = 0\n",
    "for i in range(len(X_val)):\n",
    "    prediction = np.dot(X_val[i], weights) + bias\n",
    "    if prediction >= 0:\n",
    "        y_pred = 1\n",
    "    else:\n",
    "        y_pred = -1\n",
    "    if y_pred == y_val[i]:\n",
    "        correct_val += 1\n",
    "\n",
    "validation_accuracy = correct_val / len(X_val) * 100\n",
    "print(f'Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct_test = 0\n",
    "for i in range(len(X_test)):\n",
    "    prediction = np.dot(X_test[i], weights) + bias\n",
    "    if prediction >= 0:\n",
    "        y_pred = 1\n",
    "    else:\n",
    "        y_pred = -1\n",
    "    if y_pred == y_test[i]:\n",
    "        correct_test += 1\n",
    "\n",
    "test_accuracy = correct_test / len(X_test) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9659d83-270a-4a29-8457-9e860d15f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73d26a6-7d2f-4b71-bea3-46de0b741a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Training Accuracy: 100.00%, Avg Loss: 0.00\n",
      "Validation Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Set your source and destination paths\n",
    "source_dir = '/home/salma-sulthana/Downloads/9.mnist_data(2)/9/'\n",
    "train_dir = 'train1'\n",
    "test_dir = 'test1'\n",
    "val_dir = 'validation1'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Set the percentage of data for train, test, and validation\n",
    "train_split = 0.7\n",
    "test_split = 0.2\n",
    "val_split = 0.1\n",
    "\n",
    "# Iterate through the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    # Split files into train, test, and validation sets\n",
    "    num_files = len(files)\n",
    "    train_end = int(train_split * num_files)\n",
    "    test_end = int((train_split + test_split) * num_files)\n",
    "\n",
    "    train_files = files[:train_end]\n",
    "    test_files = files[train_end:test_end]\n",
    "    val_files = files[test_end:]\n",
    "\n",
    "    # Move files to respective directories\n",
    "    for file in train_files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        shutil.move(file_path, os.path.join(train_dir, file))\n",
    "    \n",
    "    for file in test_files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        shutil.move(file_path, os.path.join(test_dir, file))\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        shutil.move(file_path, os.path.join(val_dir, file))\n",
    "\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/salma-sulthana/Downloads/salma.sulthana/train1\"\n",
    "output_folder_2 = \"/home/salma-sulthana/Downloads/salma.sulthana/test1\"\n",
    "output_folder_3 = \"/home/salma-sulthana/Downloads/salma.sulthana/validation1\"\n",
    "\n",
    "# Initialize perceptron parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Initialize perceptron weights and bias\n",
    "num_inputs = np.prod(input_shape)\n",
    "weights = np.zeros(num_inputs)\n",
    "bias = 0.0\n",
    "\n",
    "# Function to load data from folders\n",
    "def load_data(folder_path, input_shape):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "                X.append(img.flatten())\n",
    "                y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_data(output_folder_1, input_shape)\n",
    "\n",
    "# Training the perceptron\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        error = y_train[i] - prediction\n",
    "        total_loss += error ** 2  # Sum of squared errors (SSE)\n",
    "        weights += learning_rate * error * X_train[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "    # Calculate training accuracy and loss after each epoch\n",
    "    correct_train = 0\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y_train[i]:\n",
    "            correct_train += 1\n",
    "    training_accuracy = correct_train / len(X_train)\n",
    "    average_loss = total_loss / len(X_train)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {training_accuracy * 100:.2f}%, Avg Loss: {average_loss:.2f}\")\n",
    "\n",
    "# Function to test accuracy on a dataset\n",
    "def test_accuracy(X, y, weights, bias):\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        linear_output = np.dot(weights, X[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(X)\n",
    "\n",
    "# Load validation data\n",
    "X_val, y_val = load_data(output_folder_2, input_shape)\n",
    "\n",
    "# Validate and print accuracy\n",
    "validation_accuracy = test_accuracy(X_val, y_val, weights, bias)\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load testing data\n",
    "X_test, y_test = load_data(output_folder_3, input_shape)\n",
    "\n",
    "# Test and print accuracy\n",
    "test_accuracy = test_accuracy(X_test, y_test, weights, bias)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1321-1291-4626-8f47-8650757dfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "project 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c99aff2-c0b0-492c-8ac5-088faf4b74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Dropout (0.5)...\n",
      "\n",
      "Training for 5 epochs...\n",
      "\n",
      "Epoch 1, Loss: 0.1479\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Training Accuracy: 100.00 %\n",
      "Validation Loss: 0.0033, Validation Accuracy: 100.00 %\n",
      "Test Accuracy: 100.00 %\n",
      "\n",
      "Training with L2 Regularization...\n",
      "\n",
      "Training for 5 epochs...\n",
      "\n",
      "Epoch 1, Loss: 0.1191\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Training Accuracy: 100.00 %\n",
      "Validation Loss: 0.0033, Validation Accuracy: 100.00 %\n",
      "Test Accuracy: 100.00 %\n",
      "\n",
      "Training with L1 Regularization...\n",
      "\n",
      "Training for 5 epochs...\n",
      "\n",
      "Epoch 1, Loss: 0.1462\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Training Accuracy: 100.00 %\n",
      "Validation Loss: 0.0033, Validation Accuracy: 100.00 %\n",
      "Test Accuracy: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Define paths\n",
    "train_path = '/home/salma-sulthana/ann project/train/'\n",
    "val_path = '/home/salma-sulthana/ann project/validation/'\n",
    "test_path = '/home/salma-sulthana/ann project/test/'\n",
    "\n",
    "# Transform for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "images_list = []\n",
    "labels_list = []\n",
    "paths = [train_path, val_path, test_path]\n",
    "for path in paths:\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img = Image.open(os.path.join(path, filename)).convert('RGB')\n",
    "            img = transform(img)\n",
    "            img_array = img.numpy()\n",
    "            images.append(img_array)\n",
    "            labels.append(1 if filename.startswith('flower') else 0)\n",
    "    images_list.append(images)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = list(zip(images_list[0], labels_list[0]))\n",
    "val_dataset = list(zip(images_list[1], labels_list[1]))\n",
    "test_dataset = list(zip(images_list[2], labels_list[2]))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a simple CNN\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 8 * 8, 128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Regularization techniques to test\n",
    "regularization_techniques = {\n",
    "    'Dropout (0.5)': (0.5, 0.0),\n",
    "    'L2 Regularization': (0.1, 0.0),\n",
    "    'L1 Regularization': (0.01, 0.0),\n",
    "}\n",
    "\n",
    "# Different epochs to test\n",
    "epochs_list = [5]\n",
    "\n",
    "for technique, (dropout_rate, weight_decay) in regularization_techniques.items():\n",
    "    print(f'\\nTraining with {technique}...\\n')\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 8 * 8, 128),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(128, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=weight_decay)\n",
    "    for epochs in epochs_list:\n",
    "        print(f'Training for {epochs} epochs...\\n')\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                # L1 regularization\n",
    "                if weight_decay > 0:\n",
    "                    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                    loss += weight_decay * l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "        # Evaluate on training set\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels.float()).sum().item()\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print(f'Training Accuracy: {train_accuracy:.2f} %')\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.01\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels.float()).sum().item()\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f} %')\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels.float()).sum().item()\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        print(f'Test Accuracy: {test_accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15932d-0b50-4b21-bbc6-d52099430cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee558e1f-ec21-4454-806e-847a16b00d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer...\n",
      "Epoch 1, Loss: 1.6393743024789729, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 2.7939657254449912e-09, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with SGD optimizer: 100.0%\n",
      "Training with Adam optimizer...\n",
      "Epoch 1, Loss: 3.477708337022341, Train Accuracy: 99.34533551554829%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.038396374322472046, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.14881480857729912, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.083619642467238, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.053676222515059636, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adam optimizer: 100.0%\n",
      "Training with RMSprop optimizer...\n",
      "Epoch 1, Loss: 2.641121244430542, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.10243510007858277, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.028353846073150633, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with RMSprop optimizer: 100.0%\n",
      "Training with Adagrad optimizer...\n",
      "Epoch 1, Loss: 7.320324850082398, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.020292025804519654, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.1399134635925293, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adagrad optimizer: 100.0%\n",
      "Training with Adadelta optimizer...\n",
      "Epoch 1, Loss: 8.247614861279725, Train Accuracy: 99.83633387888707%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.09336189064142672, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 4.528239369392395e-05, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.00038617076352238656, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.07458170707651135, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adadelta optimizer: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define paths to datasets\n",
    "training_path = '/home/salma-sulthana/ann project/train/'\n",
    "validation_path = '/home/salma-sulthana/ann project/validation/'\n",
    "testing_path = '/home/salma-sulthana/ann project/test/'\n",
    "\n",
    " \n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Paths to datasets\n",
    "datasets = [\n",
    "    {'name': 'train', 'path': training_path},\n",
    "    {'name': 'validation', 'path': validation_path},\n",
    "    {'name': 'test', 'path': testing_path}\n",
    "]\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "data_loaders = {}\n",
    "for dataset in datasets:\n",
    "    images = [f for f in os.listdir(dataset['path']) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(dataset['path'], img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(torch.tensor(0))  # Assuming all labels are 0 for simplicity\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    data_loaders[dataset['name']] = DataLoader(TensorDataset(data_tensor, labels_tensor), batch_size=64, shuffle=dataset['name'] == 'train')\n",
    "\n",
    "# Neural network model\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(224 * 224 * 3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# List of optimization algorithms to compare\n",
    "optimizers = [\n",
    "    {'name': 'SGD', 'optimizer': optim.SGD(net.parameters(), lr=0.01, momentum=0.9)},\n",
    "    {'name': 'Adam', 'optimizer': optim.Adam(net.parameters(), lr=0.001)},\n",
    "    {'name': 'RMSprop', 'optimizer': optim.RMSprop(net.parameters(), lr=0.001)},\n",
    "    {'name': 'Adagrad', 'optimizer': optim.Adagrad(net.parameters(), lr=0.01)},\n",
    "    {'name': 'Adadelta', 'optimizer': optim.Adadelta(net.parameters(), lr=1.0)}\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "for optimizer_info in optimizers:\n",
    "    optimizer_name = optimizer_info['name']\n",
    "    optimizer = optimizer_info['optimizer']\n",
    "    print(f\"Training with {optimizer_name} optimizer...\")\n",
    "\n",
    "    # Reset model parameters\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Dropout):\n",
    "            module.p = 0.5\n",
    "\n",
    "    # Train the network\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        net.train()  # Set the model to train mode\n",
    "        for dataset_name in ['train']:\n",
    "            for i, (inputs, labels) in enumerate(data_loaders[dataset_name], 0):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loaders['train']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in data_loaders['validation']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            validation_accuracy = 100 * correct / total\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loaders[\"train\"])}, Train Accuracy: {train_accuracy}%, Validation Accuracy: {validation_accuracy}%')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loaders['test']:\n",
    "            inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy with {optimizer_name} optimizer: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4a49b-1858-4e5b-a93d-cec3d0489141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
